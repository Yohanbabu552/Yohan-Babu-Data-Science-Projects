{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aSNbH0ie_2"
      },
      "source": [
        "\n",
        "\n",
        "### 1. Importing Required Libraries\n",
        "Begin by importing the necessary libraries for data processing, splitting, and model building. These include tools for handling data, tokenizing text, padding sequences, and defining the LSTM model.\n",
        "\n",
        "### Key Components:\n",
        "- Libraries for data manipulation and preprocessing (e.g., pandas).\n",
        "- Tokenization and sequence padding to handle text input.\n",
        "- Neural network layers, including Embedding and LSTM, for building the sentiment analysis model.\n",
        "- Train-test splitting functionality to prepare data for training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5i7_KxERb4F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UniWrfQpiyAB"
      },
      "source": [
        "### 2. Loading the Dataset\n",
        "The IMDB dataset, which contains labeled text reviews, is loaded for the sentiment analysis task.\n",
        "\n",
        "### Dataset Details:\n",
        "- **Source**: The IMDB dataset.\n",
        "- **Encoding**: UTF-8 to ensure proper handling of text data.\n",
        "- **Structure**: The dataset typically consists of two columns:\n",
        "  - `review`: Contains the text of the reviews.\n",
        "  - `sentiment`: Labels indicating the sentiment (e.g., positive or negative).\n",
        "\n",
        "This dataset serves as the input for preprocessing and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z54ZvZXrSS0n"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/IMDB Dataset.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFqm9eo7i-WE"
      },
      "source": [
        "### 3. Exploring the Dataset\n",
        "After loading the dataset, we can inspect the first few rows to understand its structure and get a glimpse of the data.\n",
        "\n",
        "### Action:\n",
        "- The `head()` function is used to display the first five rows of the dataset.\n",
        "- This helps us verify the data format and check the presence of any missing or unusual values.\n",
        "\n",
        "### Example Output:\n",
        "The dataset should display two columns:\n",
        "- **review**: The text content of the review.\n",
        "- **sentiment**: The sentiment label, which typically could be 'positive' or 'negative'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UfP8uj44VWuW",
        "outputId": "659d3cc5-598d-4be9-b6fb-8a20c96fe8b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-76ddb345-50cb-402b-bc5e-f78dd7668974\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76ddb345-50cb-402b-bc5e-f78dd7668974')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-76ddb345-50cb-402b-bc5e-f78dd7668974 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-76ddb345-50cb-402b-bc5e-f78dd7668974');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b9793016-f12a-4e88-b1fc-e662c0478ed1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b9793016-f12a-4e88-b1fc-e662c0478ed1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b9793016-f12a-4e88-b1fc-e662c0478ed1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7wwg9RxjG2t"
      },
      "source": [
        "### 4. Checking the Dataset Shape\n",
        "To understand the size of the dataset, we use the `shape` attribute. This reveals the number of rows and columns in the dataset.\n",
        "\n",
        "### Action:\n",
        "- The `shape` function provides the dimensions of the dataset, i.e., the number of samples (rows) and features (columns).\n",
        "\n",
        "### Example Output:\n",
        "- The output will be in the form `(num_rows, num_columns)`, indicating the total number of data points and features.\n",
        "- This helps confirm the dataset's size and structure before moving forward with further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26DI_SB4WNGM",
        "outputId": "75541c6d-d5c9-4dbe-d5dc-6c43d16e2d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RijLIREmjN7E"
      },
      "source": [
        "### 5. Checking the Distribution of Sentiment Labels\n",
        "To understand the distribution of the sentiment labels in the dataset, we use the `value_counts()` function.\n",
        "\n",
        "### Action:\n",
        "- The `value_counts()` function is applied to the `sentiment` column to count how many instances belong to each class (e.g., positive or negative).\n",
        "- This step helps us assess whether the dataset is balanced or imbalanced with respect to the target classes.\n",
        "\n",
        "### Example Output:\n",
        "The output will display the count of each sentiment label:\n",
        "- **Positive**: Number of positive reviews.\n",
        "- **Negative**: Number of negative reviews.\n",
        "\n",
        "This step is crucial for deciding if any class balancing techniques are necessary during preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "-Zgy2O_6WPK6",
        "outputId": "b61575d8-8bc7-4676-e70d-6dfefc2f664b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "sentiment\n",
              "positive    25000\n",
              "negative    25000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivARBK11jUug"
      },
      "source": [
        "### 6. Encoding the Sentiment Labels\n",
        "The sentiment labels, which are categorical (e.g., positive, negative), are transformed into numerical values for model training using label encoding.\n",
        "\n",
        "### Action:\n",
        "- The `LabelEncoder` from scikit-learn is used to convert the sentiment labels into numerical format.\n",
        "- The `fit_transform()` function is applied to the `sentiment` column, mapping each unique label (e.g., positive, negative) to an integer value.\n",
        "\n",
        "### Example:\n",
        "- The original labels ('positive', 'negative') might be encoded as 0 and 1, respectively.\n",
        "- This transformation is necessary since machine learning models generally require numerical input.\n",
        "\n",
        "### Benefit:\n",
        "This step enables the model to interpret the sentiment as numeric values, which are essential for training the machine learning model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYsoM06oZR4n"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "df['sentiment']=le.fit_transform(df['sentiment'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTNXuXsjbWG"
      },
      "source": [
        "### 7. Splitting the Data into Features and Target\n",
        "Next, we separate the features (input variables) and the target variable (sentiment labels) for model training.\n",
        "\n",
        "### Action:\n",
        "- `X`: The features, which consist of all columns except the target column (`sentiment`). This is done using the `drop()` function.\n",
        "- `Y`: The target variable, which is the `sentiment` column that we aim to predict.\n",
        "\n",
        "### Explanation:\n",
        "- **X (Features)**: The input data used by the model to make predictions.\n",
        "- **Y (Target)**: The target labels (encoded sentiment values) that the model will learn to predict.\n",
        "\n",
        "This step prepares the data for the next phase of splitting into training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WMriNXLb2tT"
      },
      "outputs": [],
      "source": [
        "X=df.drop('sentiment',axis=1)\n",
        "Y=df['sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1zDvrEjkfH"
      },
      "source": [
        "### 8. Splitting the Data into Training and Test Sets\n",
        "To evaluate the performance of the model, the dataset is divided into training and test sets.\n",
        "\n",
        "### Action:\n",
        "- The `train_test_split()` function from scikit-learn is used to randomly split the data into training and testing subsets.\n",
        "- **X_train, Y_train**: The features and target for training the model.\n",
        "- **X_test, Y_test**: The features and target for evaluating the model’s performance.\n",
        "- The `test_size=0.2` parameter indicates that 20% of the data will be used for testing, while 80% will be used for training.\n",
        "- `random_state=42` ensures reproducibility by fixing the random seed.\n",
        "\n",
        "### Benefit:\n",
        "This step allows the model to be trained on one portion of the data (training set) and evaluated on another portion (test set), ensuring that the model’s performance is generalized and not overfitting to the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEC7pTQ7WYzV"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyRqzj2LjrVz"
      },
      "source": [
        "### 9. Verifying the Shape of Training and Test Sets\n",
        "After splitting the data, we check the dimensions of the training and test datasets to ensure the split is correct.\n",
        "\n",
        "### Action:\n",
        "- The `shape` function is applied to `train_data` and `test_data` (which should be `X_train`, `X_test`, `Y_train`, and `Y_test` if using the previous naming convention).\n",
        "- This step confirms the number of samples in both the training and test sets, helping to verify that the data split was done properly.\n",
        "\n",
        "### Example:\n",
        "- `train_data.shape` will show the number of rows and columns in the training data.\n",
        "- `test_data.shape` will show the number of rows and columns in the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0oL7DiMtf3C",
        "outputId": "35fe94be-895c-4ccc-8d4c-8b05282cc343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 1) (10000, 1) (40000,) (10000,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiyJHK0-j16q"
      },
      "source": [
        "### 10. Tokenizing and Padding Text Data\n",
        "To prepare the text data for the LSTM model, we tokenize the text (convert words into numerical representations) and then pad the sequences to ensure uniform length.\n",
        "\n",
        "### Action:\n",
        "- **Tokenizer**: The `Tokenizer` from Keras is initialized with a `num_words=5000` parameter, which limits the tokenization to the 5000 most frequent words in the dataset.\n",
        "- **Fitting the Tokenizer**: The `fit_on_texts()` function is applied to the training data (`train_data['review']`) to build the vocabulary based on the training set.\n",
        "- **Text to Sequences**: The `texts_to_sequences()` function converts the text reviews into sequences of integers where each integer corresponds to a word in the tokenizer’s vocabulary.\n",
        "- **Padding Sequences**: The `pad_sequences()` function is used to ensure all input sequences have the same length (200 in this case). Shorter sequences are padded with zeros, and longer sequences are truncated to the specified length.\n",
        "\n",
        "### Benefit:\n",
        "- **Tokenization** converts words into numerical form so the model can process them.\n",
        "- **Padding** ensures that all sequences are of equal length, making them compatible with the LSTM model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coeI6YtZW8f5"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train['review'])\n",
        "X_train=tokenizer.texts_to_sequences(X_train['review'])\n",
        "X_test=tokenizer.texts_to_sequences(X_test['review'])\n",
        "X_train=pad_sequences(X_train,maxlen=200)\n",
        "X_test=pad_sequences(X_test,maxlen=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypEUAx7ukGNo"
      },
      "source": [
        "### 11. Displaying the Tokenized and Padded Data\n",
        "After tokenizing and padding the text data, we print the training and test data to inspect the numerical representation of the reviews.\n",
        "\n",
        "### Action:\n",
        "- The `print(X_train)` and `print(X_test)` commands display the tokenized and padded sequences of the training and test data.\n",
        "- This step allows us to check the format of the data before feeding it into the LSTM model.\n",
        "\n",
        "### Expected Output:\n",
        "- The output will show the padded sequences, where each sequence is represented as an array of integers corresponding to the words in the vocabulary.\n",
        "- Each sequence will be of length 200, with zeros padding shorter sequences and truncating longer ones.\n",
        "\n",
        "This step helps confirm that the data is properly prepared for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOTuUx22XYai",
        "outputId": "cd87bce0-5e38-4406-cfd1-caa3bfce49a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1935    1 1200 ...  205  351 3856]\n",
            " [   3 1651  595 ...   89  103    9]\n",
            " [   0    0    0 ...    2  710   62]\n",
            " ...\n",
            " [   0    0    0 ... 1641    2  603]\n",
            " [   0    0    0 ...  245  103  125]\n",
            " [   0    0    0 ...   70   73 2062]]\n",
            "[[   0    0    0 ...  995  719  155]\n",
            " [  12  162   59 ...  380    7    7]\n",
            " [   0    0    0 ...   50 1088   96]\n",
            " ...\n",
            " [   0    0    0 ...  125  200 3241]\n",
            " [   0    0    0 ... 1066    1 2305]\n",
            " [   0    0    0 ...    1  332   27]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdkbtTuUkO29"
      },
      "source": [
        "### 12. Building the LSTM Model\n",
        "The model is built using the Sequential API in Keras. It consists of an embedding layer, an LSTM layer, and a dense output layer.\n",
        "\n",
        "### Action:\n",
        "- **Sequential Model**: The `Sequential()` function is used to initialize the model, which allows layers to be stacked on top of each other.\n",
        "- **Embedding Layer**: The `Embedding()` layer is added as the first layer to convert integer-encoded words into dense vector representations. It has:\n",
        "  - `5000`: The size of the vocabulary (number of unique words).\n",
        "  - `128`: The size of the embedding vectors (dimensionality).\n",
        "  - `input_length=200`: The length of the input sequences (padded to 200 words).\n",
        "- **LSTM Layer**: The `LSTM()` layer is used to capture the sequential dependencies in the text data. It has:\n",
        "  - `128`: The number of LSTM units (neurons).\n",
        "  - `dropout=0.2`: Dropout rate to prevent overfitting.\n",
        "  - `recurrent_dropout=0.2`: Dropout rate for the recurrent connections within the LSTM.\n",
        "- **Dense Layer**: The `Dense()` layer is the output layer with a single neuron and a sigmoid activation function, suitable for binary classification tasks (positive or negative sentiment).\n",
        "\n",
        "### Model Summary:\n",
        "- The `summary()` function displays the architecture of the model, including the number of layers, the number of parameters in each layer, and the total number of parameters in the model.\n",
        "\n",
        "### Model Architecture:\n",
        "- The output of this step will show the complete structure of the LSTM model with details about each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "gSxaXZyeX2BN",
        "outputId": "61cc83c5-2c18-4640-cc18-94259455d4af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(5000,128,input_length=200))\n",
        "model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPvd4mlgkWMH"
      },
      "source": [
        "### 13. Compiling the Model\n",
        "After building the model, the next step is to compile it. This configures the model for training by specifying the loss function, optimizer, and evaluation metrics.\n",
        "\n",
        "### Action:\n",
        "- **Loss Function**:\n",
        "  - `binary_crossentropy`: This is used for binary classification problems, where the target variable has two classes (e.g., positive and negative sentiment).\n",
        "- **Optimizer**:\n",
        "  - `adam`: The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp. It is widely used due to its efficiency and low memory requirements.\n",
        "- **Metrics**:\n",
        "  - `accuracy`: The accuracy metric will be used to evaluate the model’s performance during training and testing.\n",
        "\n",
        "### Benefit:\n",
        "Compiling the model sets up the necessary components for training, ensuring that the model is ready to learn from the data and optimize for the specified objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o8yfx9UYcUn"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y85mRJEWkYAi"
      },
      "source": [
        "### 14. Training the Model\n",
        "After compiling the model, we train it using the training data. During training, the model learns to predict sentiment from the review text.\n",
        "\n",
        "### Action:\n",
        "- **Model Training**: The `fit()` function is used to train the model on the training data.\n",
        "  - `X_train`: The input features (tokenized and padded text data).\n",
        "  - `Y_train`: The target variable (encoded sentiment labels).\n",
        "  - `epochs=5`: The number of times the entire training dataset will be passed through the model. Each epoch represents one complete pass through the training data.\n",
        "  - `batch_size=64`: The number of samples per gradient update. The model will update its weights after processing 64 samples at a time.\n",
        "  - `validation_split=0.2`: A portion of the training data (20%) is set aside for validation during training. This helps monitor the model’s performance on unseen data during training.\n",
        "\n",
        "### Benefit:\n",
        "Training the model enables it to learn the relationship between the text features and sentiment labels. By using validation data, we can monitor the model's performance and adjust as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBMTDyT5Ykn6",
        "outputId": "1f4b9e45-2caf-436d-a109-4eeee3ef4571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 509ms/step - accuracy: 0.7180 - loss: 0.5329 - val_accuracy: 0.8371 - val_loss: 0.3831\n",
            "Epoch 2/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 508ms/step - accuracy: 0.8511 - loss: 0.3637 - val_accuracy: 0.8421 - val_loss: 0.3784\n",
            "Epoch 3/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 496ms/step - accuracy: 0.8563 - loss: 0.3438 - val_accuracy: 0.8514 - val_loss: 0.3717\n",
            "Epoch 4/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 496ms/step - accuracy: 0.8739 - loss: 0.3194 - val_accuracy: 0.8501 - val_loss: 0.3603\n",
            "Epoch 5/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 495ms/step - accuracy: 0.8915 - loss: 0.2758 - val_accuracy: 0.8689 - val_loss: 0.3352\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b2ea26b63e0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train,Y_train,epochs=5,batch_size=64,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc9tiLClkopM"
      },
      "source": [
        "### 15. Evaluating the Model\n",
        "After training the model, we evaluate its performance on the test set to determine how well it generalizes to unseen data.\n",
        "\n",
        "### Action:\n",
        "- The `evaluate()` function is used to assess the model’s performance on the test data (`X_test` and `Y_test`).\n",
        "  - `loss`: The value of the loss function, which indicates how well the model's predictions match the true labels. A lower loss value indicates better performance.\n",
        "  - `accuracy`: The accuracy metric shows the proportion of correct predictions. Higher accuracy indicates better model performance.\n",
        "\n",
        "### Example Output:\n",
        "- `Test Loss`: Displays the loss value on the test data.\n",
        "- `Test Accuracy`: Displays the accuracy achieved on the test data.\n",
        "\n",
        "### Benefit:\n",
        "Evaluating the model helps us assess its effectiveness and determine if further improvements or adjustments are necessary before deploying it for sentiment prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5_Cap62ZNHQ",
        "outputId": "c0d72d05-bd81-4d5b-bfac-4591fc455525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 109ms/step - accuracy: 0.8728 - loss: 0.3169\n",
            "Test Loss: 0.3139933943748474\n",
            "Test Accuracy: 0.8748000264167786\n"
          ]
        }
      ],
      "source": [
        "loss,accuracy=model.evaluate(X_test,Y_test)\n",
        "print(\"Test Loss:\",loss)\n",
        "print(\"Test Accuracy:\",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27yLkSHYkyUS"
      },
      "source": [
        "### 16. Building the Predictive System\n",
        "Once the model is trained and evaluated, we can create a function to predict the sentiment of new, unseen reviews.\n",
        "\n",
        "### Action:\n",
        "- **Predict Sentiment Function**: The `predict_sentiment()` function takes a single review as input and predicts its sentiment (positive or negative).\n",
        "  - **Tokenization**: The review is first converted into a sequence of integers using the `texts_to_sequences()` function of the tokenizer.\n",
        "  - **Padding**: The sequence is then padded to ensure it has the same length as the sequences used during training (200 in this case).\n",
        "  - **Prediction**: The `predict()` function is used to generate a prediction based on the padded sequence.\n",
        "  - **Interpretation**: If the predicted value is greater than 0.5, the sentiment is classified as \"Positive Review\". Otherwise, it is classified as \"Negative Review\".\n",
        "\n",
        "### Example:\n",
        "- When a user inputs a review, the model will output whether the review is positive or negative based on the sentiment learned during training.\n",
        "\n",
        "### Benefit:\n",
        "This predictive system allows the trained model to be used in real-world applications where users can input reviews and get an instant sentiment classification (positive or negative).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z2DgRZO7dfrB"
      },
      "outputs": [],
      "source": [
        "#building a predictive system\n",
        "def predict_sentiment(review):\n",
        "  sequence=tokenizer.texts_to_sequences([review])\n",
        "  padded_sequence=pad_sequences(sequence,maxlen=200)\n",
        "  prediction=model.predict(padded_sequence)\n",
        "  if prediction>0.5:\n",
        "    print(\"Positive Review\")\n",
        "  else:\n",
        "    print(\"Negative Review\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUdd33Xkk69t"
      },
      "source": [
        "### 17. Predicting the Sentiment of a New Review\n",
        "Once the predictive system is in place, we can test it by providing a new review and observing the model’s sentiment prediction.\n",
        "\n",
        "### Action:\n",
        "- The new review, `\"This movie was Fantastic and Mindblowing\"`, is passed into the `predict_sentiment()` function.\n",
        "- The function processes the review and outputs whether the sentiment is \"Positive Review\" or \"Negative Review\".\n",
        "\n",
        "### Example Output:\n",
        "- Given that the review contains positive words like \"Fantastic\" and \"Mindblowing\", the model will likely classify it as a \"Positive Review\".\n",
        "\n",
        "### Benefit:\n",
        "This step demonstrates the model's ability to make predictions on new data and provides an immediate sentiment classification for user input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O-wCJuCSeYHA",
        "outputId": "07088d0d-5def-4231-8b69-8187c5b44aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
            "Positive Review\n"
          ]
        }
      ],
      "source": [
        "new_review=\"This movie was Fantastic and Mindblowing\"\n",
        "predict_sentiment(new_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WANK6bKpewe5",
        "outputId": "39fb2e11-0599-4820-a513-f18075a95711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Negative Review\n"
          ]
        }
      ],
      "source": [
        "new_review1=\"This movie was not that much of good.The concept of movie was very bad\"\n",
        "predict_sentiment(new_review1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VKPmiMG1fFcp"
      },
      "outputs": [],
      "source": [
        "# Assuming 'tokenizer' is your trained tokenizer\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer.to_json())  # Save the tokenizer in JSON format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sQF129p63aWX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWX1mDu+3ELVHyDjuChFdW"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}